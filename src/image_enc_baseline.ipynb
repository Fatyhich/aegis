{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77d2108",
   "metadata": {},
   "source": [
    "### Testing approach with SAM + Florence + Dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoImageProcessor,\n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.float16).__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEEF)\n",
    "\n",
    "def show_anns(anns, borders=True):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "        img[m] = color_mask \n",
    "        if borders:\n",
    "            import cv2\n",
    "            contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "            # Try to smooth contours\n",
    "            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "            cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abc7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_florence_caption(image_crop, model,\n",
    "                         processor, device,\n",
    "                         task=\"<CAPTION>\"):\n",
    "    \"\"\"Получить caption от Florence-2\"\"\"\n",
    "    inputs = processor(text=task, images=image_crop, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        num_beams=3,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed = processor.post_process_generation(\n",
    "        generated_text, task=task,\n",
    "        image_size=(image_crop.width, image_crop.height)\n",
    "    )\n",
    "\n",
    "    return parsed[task]\n",
    "\n",
    "\n",
    "def get_dinov2_embedding(image_crop, model,\n",
    "                         processor, device):\n",
    "    \"\"\"Получить CLS token от DINOv2\"\"\"\n",
    "    inputs = processor(images=image_crop, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    return outputs.last_hidden_state[0, 0].cpu().numpy()\n",
    "\n",
    "\n",
    "def get_dinov2_embeddings_batch(image_crops, model,\n",
    "                                processor, device, batch_size=8):\n",
    "    \"\"\"Получить эмбеддинги батчами (заготовка для оптимизации)\"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(image_crops), batch_size):\n",
    "        batch = image_crops[i:i + batch_size]\n",
    "        inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # CLS tokens для всего батча\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def process_masks_with_features(image, masks,\n",
    "                                florence_model, florence_processor, florence_device,\n",
    "                                dino_model, dino_processor, dino_device,\n",
    "                                use_batch=False, batch_size=8):\n",
    "    \"\"\"Обработать маски: добавить description и embedding\"\"\"\n",
    "\n",
    "    # Собираем все crops\n",
    "    crops = []\n",
    "    for mask_data in masks:\n",
    "        x, y, w, h = mask_data['bbox']\n",
    "        crop = image.crop((x, y, x + w, y + h))\n",
    "        crops.append(crop)\n",
    "\n",
    "    # Florence captions (пока без батчинга)\n",
    "    print(\"Generating captions...\")\n",
    "    for idx, (mask_data, crop) in enumerate(tqdm(zip(masks, crops), total=len(masks))):\n",
    "        mask_data['description'] = get_florence_caption(\n",
    "            crop, florence_model, florence_processor, florence_device\n",
    "        )\n",
    "\n",
    "    # DINOv2 embeddings (с опциональным батчингом)\n",
    "    print(\"Extracting embeddings...\")\n",
    "    if use_batch:\n",
    "        embeddings = get_dinov2_embeddings_batch(\n",
    "            crops, dino_model, dino_processor, dino_device, batch_size\n",
    "        )\n",
    "        for mask_data, embedding in zip(masks, embeddings):\n",
    "            mask_data['embedding'] = embedding\n",
    "    else:\n",
    "        for mask_data, crop in tqdm(zip(masks, crops), total=len(masks)):\n",
    "            mask_data['embedding'] = get_dinov2_embedding(\n",
    "                crop, dino_model, dino_processor, dino_device\n",
    "            )\n",
    "\n",
    "    return masks\n",
    "\n",
    "def visualize_masks_with_descriptions(image, masks, figsize=(25, 20)):\n",
    "    \"\"\"Визуализация масок с номерами и списком описаний\"\"\"\n",
    "    fig, (ax_img, ax_text) = plt.subplots(1, 2, figsize=figsize,\n",
    "                                            gridspec_kw={'width_ratios': [3, 1]})\n",
    "\n",
    "    # Изображение с масками и номерами\n",
    "    ax_img.imshow(image)\n",
    "    plt.sca(ax_img)\n",
    "    show_anns(masks)\n",
    "\n",
    "    # Добавляем номера в центре каждой маски\n",
    "    for idx, mask_data in enumerate(masks):\n",
    "        x, y, w, h = mask_data['bbox']\n",
    "        center_x = x + w / 2\n",
    "        center_y = y + h / 2\n",
    "\n",
    "        ax_img.text(center_x, center_y, str(idx),\n",
    "                    color='white', fontsize=14, weight='bold',\n",
    "                    ha='center', va='center',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='black', alpha=0.6))\n",
    "\n",
    "    ax_img.axis('off')\n",
    "\n",
    "    # Список описаний\n",
    "    ax_text.axis('off')\n",
    "    descriptions = \"\\n\\n\".join([f\"{i}: {mask['description']}\" for i, mask in enumerate(masks)])\n",
    "    ax_text.text(0.05, 0.50, descriptions,\n",
    "                fontsize=11, va='center', ha='left',\n",
    "                family='monospace', wrap=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bef666",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df935889",
   "metadata": {},
   "outputs": [],
   "source": [
    "projectDir = Path.cwd().parent\n",
    "dataDir = projectDir / \"data/scand_spot_fountain-lib/output_frames\"\n",
    "\n",
    "frames = sorted(dataDir.glob(\"frame_*.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa0046",
   "metadata": {},
   "source": [
    "### SAM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, str(projectDir / \"segment-anything\"))\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"/mnt/vol0/weights/sam/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=4,\n",
    "    pred_iou_thresh=0.95,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8e6b4",
   "metadata": {},
   "source": [
    "### SAM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de18b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0, str(projectDir / \"sam2\"))\n",
    "# from sam2.build_sam import build_sam2\n",
    "# from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "# sam2_checkpoint = str(projectDir / \"sam2/checkpoints/sam2.1_hiera_large.pt\")\n",
    "# model_cfg = 'configs/sam2.1/sam2.1_hiera_l.yaml'\n",
    "\n",
    "# sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "# mask_generator = SAM2AutomaticMaskGenerator(sam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48935bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "\n",
    "fl_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Florence-2-large-ft\",\n",
    "    dtype=torch_dtype,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"\n",
    ").to(device)\n",
    "\n",
    "fl_processor = AutoProcessor.from_pretrained(\n",
    "    \"microsoft/Florence-2-large-ft\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "dino_model = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device)\n",
    "dino_model.eval()\n",
    "dino_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\", use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961988ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_db = []\n",
    "\n",
    "for index, frame in enumerate(frames):\n",
    "    if index == 0: continue\n",
    "    print(f\"Frame {index}: {frame}\")\n",
    "    image = Image.open(frame).convert(\"RGB\")\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    masks = mask_generator.generate(np.array(image))\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(image)\n",
    "    show_anns(masks)\n",
    "    plt.axis('off')\n",
    "    plt.show() \n",
    "\n",
    "    masks = process_masks_with_features(\n",
    "        image, masks,\n",
    "        fl_model, fl_processor, device,\n",
    "        dino_model, dino_processor, device\n",
    "    )\n",
    "\n",
    "    visualize_masks_with_descriptions(image, masks)\n",
    "    mask_db.append((frame.name, masks))\n",
    "    \n",
    "    if index == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a573d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "\n",
    "# 1. Берем только первые 2 кадра\n",
    "frame0_embeddings = np.array([mask['embedding'] for mask in mask_db[0][1]])\n",
    "frame1_embeddings = np.array([mask['embedding'] for mask in mask_db[1][1]])\n",
    "\n",
    "# Объединяем\n",
    "all_embeddings = np.vstack([frame0_embeddings, frame1_embeddings])\n",
    "\n",
    "# Метки для цветов (0 = frame 0, 1 = frame 1)\n",
    "labels = np.array([0] * len(frame0_embeddings) + [1] * len(frame1_embeddings))\n",
    "colors = ['#FF6B6B', '#4ECDC4']  # красный для frame 0, бирюзовый для frame 1\n",
    "\n",
    "print(f\"Frame 0: {len(frame0_embeddings)} objects\")\n",
    "print(f\"Frame 1: {len(frame1_embeddings)} objects\")\n",
    "print(f\"Total: {len(all_embeddings)} objects, dim: {all_embeddings.shape[1]}\")\n",
    "\n",
    "# 2. Применяем редукцию размерности\n",
    "pca = PCA(n_components=2)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_embeddings)-1))\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "\n",
    "embeddings_pca = pca.fit_transform(all_embeddings)\n",
    "embeddings_tsne = tsne.fit_transform(all_embeddings)\n",
    "embeddings_umap = umap.fit_transform(all_embeddings)\n",
    "\n",
    "# 3. Визуализация\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "methods = [\n",
    "    (embeddings_pca, 'PCA', f'explained variance: {pca.explained_variance_ratio_.sum():.2%}'),\n",
    "    (embeddings_tsne, 't-SNE', ''),\n",
    "    (embeddings_umap, 'UMAP', '')\n",
    "]\n",
    "\n",
    "for ax, (embeddings_2d, title, subtitle) in zip(axes, methods):\n",
    "    # Frame 0\n",
    "    ax.scatter(embeddings_2d[labels==0, 0], embeddings_2d[labels==0, 1],\n",
    "                c=colors[0], label='Frame 0', alpha=0.7, s=100, edgecolors='black')\n",
    "    # Frame 1\n",
    "    ax.scatter(embeddings_2d[labels==1, 0], embeddings_2d[labels==1, 1],\n",
    "                c=colors[1], label='Frame 1', alpha=0.7, s=100, edgecolors='black')\n",
    "\n",
    "    ax.set_title(f'{title}\\n{subtitle}', fontsize=14, weight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Дополнительно - аннотировать номера объектов:\n",
    "# # Добавить номера объектов на график\n",
    "# for i, (x, y) in enumerate(embeddings_2d):\n",
    "#     ax.annotate(str(i), (x, y), fontsize=8, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30624182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Жадный алгоритм для matching объектов между кадрами\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def greedy_match_objects(frame0_embeddings, frame1_embeddings, threshold=0.7):\n",
    "    \"\"\"Жадное сопоставление объектов по косинусному расстоянию\"\"\"\n",
    "    sim_matrix = cosine_similarity(frame0_embeddings, frame1_embeddings)\n",
    "\n",
    "    matches = []\n",
    "    used_frame1 = set()\n",
    "\n",
    "    # Для каждого объекта из frame0\n",
    "    for i in range(len(frame0_embeddings)):\n",
    "        best_j = None\n",
    "        best_sim = threshold\n",
    "\n",
    "        for j in range(len(frame1_embeddings)):\n",
    "            if j not in used_frame1 and sim_matrix[i, j] > best_sim:\n",
    "                best_sim = sim_matrix[i, j]\n",
    "                best_j = j\n",
    "\n",
    "        if best_j is not None:\n",
    "            matches.append((i, best_j, best_sim))\n",
    "            used_frame1.add(best_j)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def visualize_object_tracking(image0, masks0, image1, masks1, matches):\n",
    "    \"\"\"Визуализация перемещения объектов со стрелками\"\"\"\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    # Frame 0\n",
    "    ax0.imshow(image0)\n",
    "    ax0.set_title('Frame 0', fontsize=16)\n",
    "    ax0.axis('off')\n",
    "\n",
    "    # Frame 1\n",
    "    ax1.imshow(image1)\n",
    "    ax1.set_title('Frame 1', fontsize=16)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Рисуем соответствия\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(matches)))\n",
    "\n",
    "    for (i, j, sim), color in zip(matches, colors):\n",
    "        # Центры bbox\n",
    "        x0, y0, w0, h0 = masks0[i]['bbox']\n",
    "        x1, y1, w1, h1 = masks1[j]['bbox']\n",
    "\n",
    "        center0 = (x0 + w0/2, y0 + h0/2)\n",
    "        center1 = (x1 + w1/2, y1 + h1/2)\n",
    "\n",
    "        # Bbox на обоих кадрах\n",
    "        rect0 = mpatches.Rectangle((x0, y0), w0, h0,\n",
    "                                    linewidth=3, edgecolor=color, facecolor='none')\n",
    "        rect1 = mpatches.Rectangle((x1, y1), w1, h1,\n",
    "                                    linewidth=3, edgecolor=color, facecolor='none')\n",
    "        ax0.add_patch(rect0)\n",
    "        ax1.add_patch(rect1)\n",
    "\n",
    "        # Номера и similarity\n",
    "        ax0.text(center0[0], center0[1], str(i),\n",
    "                color='white', fontsize=14, weight='bold',\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(boxstyle='round', facecolor=color, alpha=0.8))\n",
    "\n",
    "        ax1.text(center1[0], center1[1], f\"{j}\\n{sim:.2f}\",\n",
    "                color='white', fontsize=12, weight='bold',\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(boxstyle='round', facecolor=color, alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return matches\n",
    "\n",
    "# Использование:\n",
    "frame0_embeddings = np.array([mask['embedding'] for mask in mask_db[0][1]])\n",
    "frame1_embeddings = np.array([mask['embedding'] for mask in mask_db[1][1]])\n",
    "\n",
    "matches = greedy_match_objects(frame0_embeddings, frame1_embeddings, threshold=0.7)\n",
    "\n",
    "print(f\"Found {len(matches)} matches:\")\n",
    "for i, j, sim in matches:\n",
    "    print(f\"  Frame0[{i}] -> Frame1[{j}]: {sim:.3f}\")\n",
    "    print(f\"    '{mask_db[0][1][i]['description']}' -> '{mask_db[1][1][j]['description']}'\")\n",
    "\n",
    "# Визуализация\n",
    "image0 = Image.open(frames[1]).convert(\"RGB\")\n",
    "image1 = Image.open(frames[2]).convert(\"RGB\")\n",
    "\n",
    "visualize_object_tracking(image0, mask_db[0][1], image1, mask_db[1][1], matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vizEnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
